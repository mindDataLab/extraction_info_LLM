# Projet d'Extraction LLM vers PostgreSQL (Multi-utilisateurs)

Ce projet utilise un LLM local pour extraire des informations structur√©es depuis des articles et les enregistrer dans une base de donn√©es PostgreSQL. Il offre une interface web conviviale avec gestion des utilisateurs et un historique personnalis√©, ainsi qu'une option en ligne de commande pour le traitement par lots.

## Structure des Fichiers

```
.
‚îú‚îÄ‚îÄ app.py                # L'application web interactive (Streamlit) avec gestion des utilisateurs.
‚îú‚îÄ‚îÄ run_extraction.py     # Le script pour le traitement par lots en ligne de commande.
‚îú‚îÄ‚îÄ database.py           # Module de gestion de la base de donn√©es PostgreSQL (connexion, utilisateurs, extractions).
‚îú‚îÄ‚îÄ system_prompt.txt     # Fichier contenant les instructions (prompt) pour le LLM.
‚îú‚îÄ‚îÄ a_traiter/            # DOSSIER : Placez vos fichiers .txt pour le traitement par lots.
‚îú‚îÄ‚îÄ traites/              # DOSSIER : Les fichiers trait√©s sont d√©plac√©s ici.
‚îú‚îÄ‚îÄ .streamlit/           # DOSSIER : Contient le fichier secrets.toml pour les identifiants de la BDD.
‚îÇ   ‚îî‚îÄ‚îÄ secrets.toml      # Fichier de configuration s√©curis√© pour les identifiants PostgreSQL.
‚îî‚îÄ‚îÄ venv/                 # L'environnement virtuel Python.
```

## ‚öôÔ∏è Configuration

Suivez ces √©tapes pour configurer le projet.

### √âtape 1 : Installation et Configuration de PostgreSQL

1.  **Installer PostgreSQL** :
    *   **macOS (avec Homebrew)** : Ouvrez votre terminal et ex√©cutez `brew install postgresql`.
    *   **Autres OS** : Suivez les instructions officielles pour votre syst√®me d'exploitation.

2.  **D√©marrer le service PostgreSQL** :
    *   **macOS (avec Homebrew)** : `brew services start postgresql`.
    *   Assurez-vous que le service PostgreSQL est en cours d'ex√©cution.

3.  **Cr√©er la base de donn√©es** :
    *   Cr√©ez une base de donn√©es d√©di√©e pour le projet. Par exemple : `createdb sprint_ai_db`.

4.  **Configurer les identifiants dans `secrets.toml`** :
    *   Cr√©ez un dossier `.streamlit` √† la racine de votre projet.
    *   √Ä l'int√©rieur de ce dossier, cr√©ez un fichier nomm√© `secrets.toml`.
    *   Ajoutez-y vos informations de connexion PostgreSQL. Exemple pour une installation locale par d√©faut sur macOS :

    ```toml
    # .streamlit/secrets.toml

    [postgres]
    host = "localhost"
    port = 5432
    dbname = "sprint_ai_db"
    user = "VOTRE_NOM_UTILISATEUR_SYSTEME" # Remplacez par votre nom d'utilisateur macOS
    password = "" # Laissez vide si vous n'avez pas d√©fini de mot de passe
    ```
    *   **Important** : Remplacez `VOTRE_NOM_UTILISATEUR_SYSTEME` par votre v√©ritable nom d'utilisateur syst√®me.

### √âtape 2 : Installation des D√©pendances Python

1.  Ouvrez un terminal √† la racine du projet.
2.  Cr√©ez et activez un environnement virtuel (si ce n'est pas d√©j√† fait) :
    ```bash
    python3 -m venv venv
    source venv/bin/activate
    ```
3.  Installez toutes les biblioth√®ques n√©cessaires :
    ```bash
    pip install streamlit requests pandas psycopg2-binary bcrypt
    ```

## üöÄ Utilisation

Vous avez deux fa√ßons d'utiliser cet outil.

### Option 1 : Interface Web (Recommand√©)

C'est la m√©thode la plus simple et la plus interactive, avec gestion des utilisateurs et historique.

1.  **Lancez votre serveur LLM** (avec LM Studio, par exemple).
2.  Assurez-vous que votre environnement virtuel est activ√© (`source venv/bin/activate`).
3.  Lancez l'application Streamlit :
    ```bash
    streamlit run app.py
    ```
4.  Ouvrez l'URL locale affich√©e dans votre terminal (g√©n√©ralement `http://localhost:8501`) dans votre navigateur.
5.  **Connectez-vous** ou **cr√©ez un compte**.
6.  Dans l'onglet "Analyse d'Article", collez le texte et lancez l'extraction. Les r√©sultats seront sauvegard√©s dans votre historique personnel.
7.  Consultez vos extractions pass√©es dans l'onglet "Mon Historique" et t√©l√©chargez-les au format JSON.

### Option 2 : Ligne de Commande (Traitement par Lots)

Utilisez cette m√©thode pour traiter plusieurs fichiers d'un coup et les associer √† un utilisateur existant.

1.  Placez un ou plusieurs fichiers `.txt` dans le dossier `a_traiter`.
2.  Lancez votre serveur LLM.
3.  Depuis votre terminal (avec l'environnement activ√©), ex√©cutez le script en sp√©cifiant un nom d'utilisateur existant (cr√©√© via l'interface web) :
    ```bash
    python3 run_extraction.py --user VOTRE_NOM_UTILISATEUR
    ```
    *   **Important** : L'utilisateur sp√©cifi√© doit exister dans la base de donn√©es.

## üß† Fonctionnalit√©s

*   **Gestion des Utilisateurs** : Chaque utilisateur a son propre compte et son historique d'extractions.
*   **Historique Personnalis√©** : Acc√©dez et t√©l√©chargez vos extractions pass√©es directement depuis l'interface web.
*   **Auto-r√©paration du JSON** : Le script int√®gre un m√©canisme de r√©silience : si le LLM renvoie un JSON malform√©, il lui demande automatiquement de corriger sa propre erreur, r√©duisant ainsi les √©checs d'analyse.
*   **Prompts Personnalisables** : Chaque utilisateur peut d√©finir et sauvegarder son propre prompt syst√®me pour adapter l'extraction √† ses besoins sp√©cifiques.

## üé® Personnalisation

*   **Comportement de l'IA** : Le moyen le plus simple d'affiner l'extraction est de modifier le prompt syst√®me directement depuis l'interface web (section "Configuration" dans la barre lat√©rale). Vous pouvez aussi √©diter le fichier `system_prompt.txt` manuellement.

## üìà √âvolutions Possibles

*   **Mise √† jour des donn√©es** existantes au lieu de l'ajout syst√©matique.
*   **Analyse depuis une URL** directement dans l'interface web.
*   **Gestion des r√¥les** utilisateurs (administrateur, etc.).
*   **Interface d'administration** pour g√©rer les utilisateurs et les extractions.
*   **Scalabilit√© du LLM** : Pour g√©rer un plus grand nombre de requ√™tes simultan√©es ou des mod√®les plus lourds :
    *   **D√©ploiement sur un serveur d√©di√©** : H√©berger le LLM sur un serveur plus puissant (avec GPU si n√©cessaire) et accessible via une API.
    *   **Utilisation d'un service LLM Cloud** : Int√©grer un service de LLM externe (ex: OpenAI, Gemini API, Hugging Face Inference API) qui g√®re la scalabilit√© automatiquement, moyennant des co√ªts d'utilisation.
*   **D√©ploiement de l'application** :
    *   **Streamlit Cloud (Recommand√©)** : La solution la plus simple pour mettre votre application en ligne. N√©cessite que votre code soit sur GitHub. Vous devrez configurer vos secrets (base de donn√©es, cl√© API LLM) directement dans l'interface de Streamlit Cloud. **Attention :** Votre LLM local devra √™tre remplac√© par un service LLM cloud ou un LLM d√©ploy√© sur un serveur distant accessible via une API. Votre base de donn√©es PostgreSQL devra √©galement √™tre accessible depuis Streamlit Cloud.